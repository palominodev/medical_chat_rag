# Sistema RAG con Chat para Documentos M√©dicos

## üìã Descripci√≥n General

Este documento detalla la arquitectura y los requerimientos t√©cnicos para implementar un sistema **RAG (Retrieval-Augmented Generation)** con interfaz de chat, dise√±ado espec√≠ficamente para procesar y consultar **legajos m√©dicos** en formato PDF.

### Flujo de Funcionamiento

```mermaid
flowchart LR
    A[üìÑ Upload PDF] --> B[üìù Procesamiento]
    B --> C[üî¢ Vectorizaci√≥n]
    C --> D[(Vector DB)]
    E[üí¨ Chat Usuario] --> F[üîç B√∫squeda Sem√°ntica]
    F --> D
    D --> G[üìö Contexto Relevante]
    G --> H[ü§ñ LLM]
    H --> I[üí¨ Respuesta]
    J[üß† Memoria] --> H
```

---

## üèóÔ∏è Arquitectura del Sistema

### Componentes Principales

| Componente | Descripci√≥n | Tecnolog√≠a |
|------------|-------------|------------|
| **Frontend** | Interfaz de usuario (upload + chat) | Next.js + React |
| **Backend API** | Procesamiento y orquestaci√≥n | Next.js API Routes |
| **Procesador PDF** | Extracci√≥n de texto de PDFs | **pdf-parse** |
| **Embeddings** | Vectorizaci√≥n de texto | **Gemini** (`gemini-embedding-001`) |
| **Vector Database** | Almacenamiento de embeddings | **Supabase pgvector** |
| **LLM** | Generaci√≥n de respuestas | **Gemini** (`gemini-3-flash-preview`) |
| **Memoria** | Historial de conversaci√≥n | **Supabase** |

---

## üì¶ Stack Tecnol√≥gico

### Dependencias (Next.js + TypeScript)

```json
{
  "dependencies": {
    "@google/generative-ai": "^0.21.x",
    "@supabase/supabase-js": "^2.x",
    "pdf-parse": "^1.1.1",
    "ai": "^3.x"
  },
  "devDependencies": {
    "@types/pdf-parse": "^1.1.x"
  }
}
```

### Variables de Entorno

```env
# Gemini API
GEMINI_API_KEY=tu_api_key_de_gemini

# Supabase
NEXT_PUBLIC_SUPABASE_URL=https://tu-proyecto.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=tu_anon_key
SUPABASE_SERVICE_ROLE_KEY=tu_service_role_key
```

---

## üìÑ Procesamiento de Documentos PDF

### Consideraciones para Legajos M√©dicos (~45 p√°ginas)

> [!IMPORTANT]
> Los documentos m√©dicos pueden contener:
> - Texto estructurado (tablas de laboratorio)
> - Texto no estructurado (notas del m√©dico)
> - Im√°genes (radiograf√≠as, estudios)
> - Formularios escaneados

### Estrategia de Chunking

```typescript
interface ChunkingConfig {
  chunkSize: number;      // 1000-1500 caracteres recomendado
  chunkOverlap: number;   // 200-300 caracteres (20% del chunk)
  separator: string[];    // ["\n\n", "\n", " "]
}

const medicalChunkConfig: ChunkingConfig = {
  chunkSize: 1200,
  chunkOverlap: 250,
  separator: ["\n\n", "\n", ". "]
};
```

### Proceso de Extracci√≥n con pdf-parse

```typescript
import pdfParse from "pdf-parse";

interface ProcessedChunk {
  content: string;
  pageNumber: number;
  chunkIndex: number;
  metadata: {
    documentType: string;
    processedAt: string;
  };
}

async function processDocument(pdfBuffer: Buffer): Promise<ProcessedChunk[]> {
  // 1. Extraer texto del PDF
  const pdfData = await pdfParse(pdfBuffer);
  const fullText = pdfData.text;
  
  // 2. Dividir en chunks
  const chunks = splitIntoChunks(fullText, {
    chunkSize: 1200,
    chunkOverlap: 250
  });
  
  // 3. A√±adir metadata
  return chunks.map((content, index) => ({
    content,
    pageNumber: Math.floor(index / 3) + 1, // Estimaci√≥n
    chunkIndex: index,
    metadata: {
      documentType: "medical_record",
      processedAt: new Date().toISOString()
    }
  }));
}

// Funci√≥n de chunking manual
function splitIntoChunks(
  text: string, 
  config: { chunkSize: number; chunkOverlap: number }
): string[] {
  const { chunkSize, chunkOverlap } = config;
  const chunks: string[] = [];
  
  // Dividir por p√°rrafos primero
  const paragraphs = text.split(/\n\n+/);
  let currentChunk = "";
  
  for (const paragraph of paragraphs) {
    if ((currentChunk + paragraph).length > chunkSize) {
      if (currentChunk) {
        chunks.push(currentChunk.trim());
        // Mantener overlap
        currentChunk = currentChunk.slice(-chunkOverlap) + paragraph;
      } else {
        // P√°rrafo muy largo, dividir por oraciones
        chunks.push(paragraph.slice(0, chunkSize));
        currentChunk = paragraph.slice(chunkSize - chunkOverlap);
      }
    } else {
      currentChunk += (currentChunk ? "\n\n" : "") + paragraph;
    }
  }
  
  if (currentChunk.trim()) {
    chunks.push(currentChunk.trim());
  }
  
  return chunks;
}
```

---

## üî¢ Vectorizaci√≥n (Embeddings) - Gemini

### Modelo: `gemini-embedding-001`

> [!TIP]
> Gemini Embeddings ofrece dimensiones flexibles (128-3072) y task types espec√≠ficos para optimizar la calidad de los embeddings seg√∫n el caso de uso.

| Caracter√≠stica | Valor |
|----------------|-------|
| **Modelo** | `gemini-embedding-001` |
| **Dimensiones** | Configurables: 768, 1536, 3072 (default) |
| **L√≠mite de tokens** | 2,048 tokens por request |
| **Tipo de entrada** | Texto |
| **Costo** | Gratuito en tier b√°sico / Pay-as-you-go |

### Task Types para RAG

Gemini permite especificar el tipo de tarea para mejorar la calidad de los embeddings:

| Task Type | Uso | Descripci√≥n |
|-----------|-----|-------------|
| `RETRIEVAL_DOCUMENT` | **Indexar chunks** | Para vectorizar documentos/chunks a almacenar |
| `RETRIEVAL_QUERY` | **Buscar** | Para vectorizar la consulta del usuario |
| `QUESTION_ANSWERING` | **Q&A** | Optimizado para preguntas y respuestas |
| `SEMANTIC_SIMILARITY` | **Comparaci√≥n** | Para comparar similitud entre textos |

### C√°lculo para 45 p√°ginas

```
Estimaci√≥n:
- 45 p√°ginas √ó ~500 palabras/p√°gina = 22,500 palabras
- ~30,000 tokens aproximadamente
- Chunks resultantes: ~25-35 chunks

Costo aproximado (Gemini API):
- Free tier: 1,500 requests/d√≠a GRATIS
- Pay-as-you-go: Precio competitivo vs OpenAI
```

### Implementaci√≥n con Google Generative AI SDK

```typescript
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY!);

// Modelo de embeddings
const embeddingModel = genAI.getGenerativeModel({ 
  model: "gemini-embedding-001" 
});

// Vectorizar documentos (para indexar)
async function embedDocuments(texts: string[]): Promise<number[][]> {
  const embeddings: number[][] = [];
  
  for (const text of texts) {
    const result = await embeddingModel.embedContent({
      content: { parts: [{ text }] },
      taskType: "RETRIEVAL_DOCUMENT",
      outputDimensionality: 768  // 768, 1536, o 3072
    });
    embeddings.push(result.embedding.values);
  }
  
  return embeddings;
}

// Vectorizar query (para b√∫squeda)
async function embedQuery(query: string): Promise<number[]> {
  const result = await embeddingModel.embedContent({
    content: { parts: [{ text: query }] },
    taskType: "RETRIEVAL_QUERY",
    outputDimensionality: 768
  });
  
  return result.embedding.values;
}
```

### Procesamiento por Lotes (Batch)

```typescript
// Embeddings en batch para mejor rendimiento
async function embedBatch(texts: string[]): Promise<number[][]> {
  const contents = texts.map(text => ({
    content: { parts: [{ text }] }
  }));
  
  const result = await embeddingModel.batchEmbedContents({
    requests: contents.map(c => ({
      ...c,
      taskType: "RETRIEVAL_DOCUMENT",
      outputDimensionality: 768
    }))
  });
  
  return result.embeddings.map(e => e.values);
}
```

### Configuraci√≥n de Dimensiones

> [!NOTE]
> Gemini usa **Matryoshka Representation Learning (MRL)**, permitiendo truncar embeddings sin perder calidad significativa.

| Dimensi√≥n | Uso Recomendado | Almacenamiento |
|-----------|-----------------|----------------|
| **768** | Balance rendimiento/calidad | ~3KB/vector |
| **1536** | Alta calidad | ~6KB/vector |
| **3072** | M√°xima precisi√≥n | ~12KB/vector |

**Recomendaci√≥n para documentos m√©dicos: `768` o `1536` dimensiones.**

---

## üíæ Base de Datos Vectorial

### Opci√≥n Recomendada: Supabase con pgvector

```sql
-- Habilitar extensi√≥n pgvector
CREATE EXTENSION IF NOT EXISTS vector;

-- Tabla para documentos
CREATE TABLE medical_documents (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  filename TEXT NOT NULL,
  uploaded_at TIMESTAMP DEFAULT NOW(),
  user_id UUID REFERENCES auth.users(id),
  metadata JSONB
);

-- Tabla para chunks vectorizados
CREATE TABLE document_chunks (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  document_id UUID REFERENCES medical_documents(id) ON DELETE CASCADE,
  content TEXT NOT NULL,
  embedding VECTOR(768),  -- Usar 768, 1536 o 3072 seg√∫n configuraci√≥n de Gemini
  page_number INTEGER,
  chunk_index INTEGER,
  metadata JSONB,
  created_at TIMESTAMP DEFAULT NOW()
);

-- √çndice para b√∫squeda eficiente
CREATE INDEX ON document_chunks 
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- Funci√≥n de b√∫squeda sem√°ntica
CREATE OR REPLACE FUNCTION match_documents(
  query_embedding VECTOR(768),  -- Debe coincidir con outputDimensionality
  match_threshold FLOAT DEFAULT 0.7,
  match_count INT DEFAULT 5,
  filter_document_id UUID DEFAULT NULL
)
RETURNS TABLE (
  id UUID,
  content TEXT,
  similarity FLOAT,
  metadata JSONB
)
LANGUAGE plpgsql
AS $$
BEGIN
  RETURN QUERY
  SELECT
    dc.id,
    dc.content,
    1 - (dc.embedding <=> query_embedding) AS similarity,
    dc.metadata
  FROM document_chunks dc
  WHERE 
    (filter_document_id IS NULL OR dc.document_id = filter_document_id)
    AND 1 - (dc.embedding <=> query_embedding) > match_threshold
  ORDER BY dc.embedding <=> query_embedding
  LIMIT match_count;
END;
$$;
```

---

## üîç Sistema de Recuperaci√≥n (Retrieval)

### B√∫squeda H√≠brida Recomendada

```typescript
interface RetrievalConfig {
  semanticWeight: number;  // 0.7 - B√∫squeda por similitud
  keywordWeight: number;   // 0.3 - B√∫squeda por palabras clave
  topK: number;            // 5-10 chunks m√°s relevantes
  threshold: number;       // 0.7 similitud m√≠nima
}

async function hybridSearch(
  query: string,
  documentId: string,
  config: RetrievalConfig
): Promise<RetrievedChunk[]> {
  // 1. Vectorizar query
  const queryEmbedding = await embeddings.embedQuery(query);
  
  // 2. B√∫squeda sem√°ntica
  const semanticResults = await supabase.rpc("match_documents", {
    query_embedding: queryEmbedding,
    match_threshold: config.threshold,
    match_count: config.topK,
    filter_document_id: documentId
  });
  
  // 3. Combinar y rankear resultados
  return semanticResults.data;
}
```

---

## üß† Memoria de Conversaci√≥n (Supabase)

### Tablas para Persistir Memoria

```sql
CREATE TABLE chat_sessions (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  document_id UUID REFERENCES medical_documents(id) ON DELETE CASCADE,
  user_id UUID REFERENCES auth.users(id),
  title TEXT,
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE chat_messages (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  session_id UUID REFERENCES chat_sessions(id) ON DELETE CASCADE,
  role TEXT CHECK (role IN ('user', 'assistant', 'system')) NOT NULL,
  content TEXT NOT NULL,
  metadata JSONB,
  created_at TIMESTAMP DEFAULT NOW()
);

-- √çndice para recuperar historial r√°pidamente
CREATE INDEX idx_messages_session ON chat_messages(session_id, created_at);

-- Trigger para actualizar updated_at
CREATE OR REPLACE FUNCTION update_updated_at()
RETURNS TRIGGER AS $$
BEGIN
  NEW.updated_at = NOW();
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trigger_update_session
BEFORE UPDATE ON chat_sessions
FOR EACH ROW EXECUTE FUNCTION update_updated_at();
```

### Funciones de Memoria en TypeScript

```typescript
import { createClient } from "@supabase/supabase-js";

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
);

interface ChatMessage {
  id: string;
  role: "user" | "assistant";
  content: string;
  created_at: string;
}

// Crear nueva sesi√≥n de chat
async function createChatSession(
  documentId: string, 
  userId: string
): Promise<string> {
  const { data, error } = await supabase
    .from("chat_sessions")
    .insert({ document_id: documentId, user_id: userId })
    .select("id")
    .single();
  
  if (error) throw error;
  return data.id;
}

// Obtener historial de chat (√∫ltimos N mensajes)
async function getChatHistory(
  sessionId: string, 
  limit: number = 10
): Promise<ChatMessage[]> {
  const { data, error } = await supabase
    .from("chat_messages")
    .select("id, role, content, created_at")
    .eq("session_id", sessionId)
    .order("created_at", { ascending: true })
    .limit(limit);
  
  if (error) throw error;
  return data || [];
}

// Guardar mensaje en el historial
async function saveChatMessage(
  sessionId: string,
  role: "user" | "assistant",
  content: string
): Promise<void> {
  const { error } = await supabase
    .from("chat_messages")
    .insert({ session_id: sessionId, role, content });
  
  if (error) throw error;
}

// Formatear historial para el prompt
function formatChatHistory(messages: ChatMessage[]): string {
  return messages
    .map(msg => `${msg.role === "user" ? "Usuario" : "Asistente"}: ${msg.content}`)
    .join("\n");
}
```

### Estrategia de Ventana Deslizante

> [!NOTE]
> Para conversaciones largas, usa una ventana de los √∫ltimos **10-15 mensajes** para mantener contexto sin exceder l√≠mites de tokens.

```typescript
// Obtener contexto optimizado
async function getOptimizedHistory(sessionId: string): Promise<string> {
  // √öltimos 10 mensajes para contexto inmediato
  const recentMessages = await getChatHistory(sessionId, 10);
  
  return formatChatHistory(recentMessages);
}
```

---

## ü§ñ Generaci√≥n de Respuestas (Gemini LLM)

### Configuraci√≥n del Modelo Gemini

```typescript
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY!);

// Modelo para chat
const chatModel = genAI.getGenerativeModel({ 
  model: "gemini-3-flash-preview",
  generationConfig: {
    temperature: 0.3,      // Bajo para respuestas consistentes
    topP: 0.8,
    topK: 40,
    maxOutputTokens: 2048
  }
});
```

### Prompt Template para Asistente M√©dico

```typescript
const MEDICAL_ASSISTANT_PROMPT = `
Eres un asistente m√©dico especializado en analizar legajos m√©dicos.
Tu rol es ayudar a interpretar y responder preguntas sobre el documento m√©dico proporcionado.

## Directrices:
1. Basa TODAS tus respuestas en el contexto del documento proporcionado
2. Si la informaci√≥n no est√° en el documento, indica claramente que no est√° disponible
3. Usa terminolog√≠a m√©dica apropiada pero explica t√©rminos complejos
4. NO inventes informaci√≥n m√©dica
5. Siempre recomienda consultar con un profesional de salud para decisiones m√©dicas

## Contexto del Documento:
{context}

## Historial de Conversaci√≥n:
{chat_history}

## Pregunta del Usuario:
{question}

## Tu Respuesta:
`;
```

### Funci√≥n de Generaci√≥n con Gemini

```typescript
async function generateResponse(
  context: string,
  chatHistory: string,
  question: string
): Promise<string> {
  const prompt = MEDICAL_ASSISTANT_PROMPT
    .replace("{context}", context)
    .replace("{chat_history}", chatHistory)
    .replace("{question}", question);
  
  const result = await chatModel.generateContent(prompt);
  const response = result.response;
  
  return response.text();
}

// Versi√≥n con streaming
async function* generateResponseStream(
  context: string,
  chatHistory: string,
  question: string
): AsyncGenerator<string> {
  const prompt = MEDICAL_ASSISTANT_PROMPT
    .replace("{context}", context)
    .replace("{chat_history}", chatHistory)
    .replace("{question}", question);
  
  const result = await chatModel.generateContentStream(prompt);
  
  for await (const chunk of result.stream) {
    yield chunk.text();
  }
}
```

---

## üîÑ Flujo Completo de Chat

```typescript
async function processChat(
  userMessage: string,
  documentId: string,
  sessionId: string
): Promise<string> {
  // 1. Recuperar historial de conversaci√≥n
  const chatHistory = await getChatHistory(sessionId, 10);
  
  // 2. Buscar contexto relevante en el documento
  const relevantChunks = await hybridSearch(userMessage, documentId, {
    semanticWeight: 0.7,
    keywordWeight: 0.3,
    topK: 5,
    threshold: 0.7
  });
  
  // 3. Construir contexto
  const context = relevantChunks
    .map(chunk => chunk.content)
    .join("\n\n---\n\n");
  
  // 4. Formatear historial
  const historyText = chatHistory
    .map(msg => `${msg.role}: ${msg.content}`)
    .join("\n");
  
  // 5. Generar respuesta
  const prompt = MEDICAL_ASSISTANT_PROMPT
    .replace("{context}", context)
    .replace("{chat_history}", historyText)
    .replace("{question}", userMessage);
  
  const response = await llm.invoke(prompt);
  
  // 6. Guardar en historial
  await saveChatMessage(sessionId, "user", userMessage);
  await saveChatMessage(sessionId, "assistant", response.content);
  
  return response.content;
}
```

---

## üìê API Routes para Next.js

### Estructura de Endpoints

```
/api
‚îú‚îÄ‚îÄ /documents
‚îÇ   ‚îú‚îÄ‚îÄ POST /upload          # Subir y procesar PDF
‚îÇ   ‚îú‚îÄ‚îÄ GET /:id              # Obtener documento
‚îÇ   ‚îî‚îÄ‚îÄ DELETE /:id           # Eliminar documento
‚îú‚îÄ‚îÄ /chat
‚îÇ   ‚îú‚îÄ‚îÄ POST /message         # Enviar mensaje
‚îÇ   ‚îú‚îÄ‚îÄ GET /sessions/:docId  # Listar sesiones
‚îÇ   ‚îî‚îÄ‚îÄ GET /history/:sessId  # Obtener historial
‚îî‚îÄ‚îÄ /embeddings
    ‚îî‚îÄ‚îÄ POST /search          # B√∫squeda sem√°ntica
```

### Ejemplo: Endpoint de Chat con Gemini

```typescript
// app/api/chat/message/route.ts
import { NextRequest, NextResponse } from "next/server";
import { GoogleGenerativeAI } from "@google/generative-ai";
import { createClient } from "@supabase/supabase-js";

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY!);
const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
);

export async function POST(req: NextRequest) {
  const { message, documentId, sessionId } = await req.json();
  
  if (!message || !documentId) {
    return NextResponse.json(
      { error: "Message and documentId are required" },
      { status: 400 }
    );
  }
  
  try {
    // 1. Obtener historial
    const history = await getChatHistory(sessionId, 10);
    
    // 2. Buscar contexto relevante
    const chunks = await searchRelevantChunks(message, documentId);
    
    // 3. Generar respuesta con Gemini
    const response = await generateResponse(
      chunks.map(c => c.content).join("\n\n"),
      formatChatHistory(history),
      message
    );
    
    // 4. Guardar mensajes
    await saveChatMessage(sessionId, "user", message);
    await saveChatMessage(sessionId, "assistant", response);
    
    return NextResponse.json({ response });
  } catch (error) {
    console.error("Chat error:", error);
    return NextResponse.json(
      { error: "Failed to process message" },
      { status: 500 }
    );
  }
}
```

---

## üîí Consideraciones de Seguridad

> [!CAUTION]
> Los documentos m√©dicos contienen **informaci√≥n sensible (PHI)**. Implementa estas medidas:

### Checklist de Seguridad

- [ ] **Encriptaci√≥n en tr√°nsito**: HTTPS obligatorio
- [ ] **Encriptaci√≥n en reposo**: Encriptar PDFs almacenados
- [ ] **Control de acceso**: RLS (Row Level Security) en Supabase
- [ ] **Autenticaci√≥n**: JWT con expiraci√≥n corta
- [ ] **Auditor√≠a**: Logs de acceso a documentos
- [ ] **Anonimizaci√≥n**: Opci√≥n para ocultar datos sensibles
- [ ] **Retenci√≥n de datos**: Pol√≠tica de eliminaci√≥n autom√°tica

### Row Level Security (RLS)

```sql
-- Habilitar RLS
ALTER TABLE medical_documents ENABLE ROW LEVEL SECURITY;
ALTER TABLE document_chunks ENABLE ROW LEVEL SECURITY;
ALTER TABLE chat_sessions ENABLE ROW LEVEL SECURITY;
ALTER TABLE chat_messages ENABLE ROW LEVEL SECURITY;

-- Pol√≠ticas de acceso
CREATE POLICY "Users can view own documents"
ON medical_documents FOR SELECT
USING (auth.uid() = user_id);

CREATE POLICY "Users can insert own documents"
ON medical_documents FOR INSERT
WITH CHECK (auth.uid() = user_id);
```

---

## ‚ö° Optimizaciones de Rendimiento

### Para Documentos de 45 P√°ginas

| Optimizaci√≥n | Descripci√≥n | Impacto |
|--------------|-------------|---------|
| **Procesamiento async** | Procesar PDF en background job | UX mejorada |
| **Caching de embeddings** | No re-vectorizar documentos | Ahorro de costos |
| **Streaming responses** | Respuestas en tiempo real | UX mejorada |
| **√çndices optimizados** | IVFFlat para pgvector | B√∫squeda r√°pida |
| **Rate limiting** | Limitar requests por usuario | Protecci√≥n de costos |

### Ejemplo: Background Processing

```typescript
// Usar queue para procesamiento
import { Queue } from "bullmq";

const documentQueue = new Queue("document-processing", {
  connection: {
    host: process.env.REDIS_HOST,
    port: parseInt(process.env.REDIS_PORT || "6379")
  }
});

// A√±adir trabajo a la cola
await documentQueue.add("process-pdf", {
  documentId,
  filePath,
  userId
});
```

---

## üìä Estimaci√≥n de Costos

### Por Documento (45 p√°ginas)

| Servicio | Costo Estimado |
|----------|----------------|
| Embeddings (Gemini) | **GRATIS** (Free tier) |
| Almacenamiento (Supabase) | $0.001/mes |
| LLM (Gemini Pro) | $0.001-0.01/consulta |
| **Total por documento** | **~$0.01/d√≠a uso activo** |

### Mensual (100 documentos, 500 consultas/doc)

```
Embeddings: GRATIS (hasta 1,500 requests/d√≠a)
Storage: 100 √ó $0.001 = $0.10
LLM (Gemini Pro): 50,000 consultas √ó $0.005 = $250

Estimado mensual: ~$250-400
```

> [!TIP]
> Usando Gemini tanto para embeddings como para LLM reduces significativamente los costos comparado con OpenAI.

---

## üöÄ Pasos de Implementaci√≥n

### Fase 1: Fundamentos (Semana 1)
- [ ] Configurar proyecto Next.js
- [ ] Integrar Supabase (auth + database)
- [ ] Crear tablas y funciones SQL
- [ ] Implementar upload de PDF

### Fase 2: RAG Core (Semana 2)
- [ ] Integrar procesamiento de PDF
- [ ] Implementar chunking y embeddings
- [ ] Configurar b√∫squeda vectorial
- [ ] Crear endpoint de b√∫squeda

### Fase 3: Chat (Semana 3)
- [ ] Implementar interfaz de chat
- [ ] Integrar LLM con streaming
- [ ] A√±adir memoria de conversaci√≥n
- [ ] Persistir historial

### Fase 4: Optimizaci√≥n (Semana 4)
- [ ] Implementar background processing
- [ ] A√±adir caching
- [ ] Configurar seguridad (RLS)
- [ ] Testing y ajustes de prompts

---

## üìö Recursos Adicionales

- [Gemini Embeddings Documentation](https://ai.google.dev/gemini-api/docs/embeddings)
- [Google Generative AI SDK](https://github.com/google/generative-ai-js)
- [LangChain.js Documentation](https://js.langchain.com/)
- [Supabase Vector Guide](https://supabase.com/docs/guides/ai)
- [Vercel AI SDK](https://sdk.vercel.ai/docs)
- [Gemini API Cookbook](https://github.com/google-gemini/cookbook)

---

> [!NOTE]
> Este documento sirve como gu√≠a de referencia. La implementaci√≥n espec√≠fica puede variar seg√∫n los requerimientos del proyecto y las decisiones de arquitectura tomadas durante el desarrollo.
